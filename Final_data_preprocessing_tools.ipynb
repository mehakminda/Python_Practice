{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehakminda/Python_Practice/blob/main/Final_data_preprocessing_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Data Preprocessing Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "V_w3ujzecNLR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifying the features and dependent variable entities,\n",
        "we predict the value of dependent variable based on the feature/independent variable.\n",
        "\n",
        "\n",
        "iloc[:]/iloc[:,:] :  All rows and all columns\n",
        "iloc[0] : first row, all column\n",
        "iloc[1:4] : from row 1 to row 3, all coulmn\n",
        "iloc[:, :2] : all rows, columns at index 0,1\n",
        "iloc[:,:-1] : locate indexes .. all row, all column except last one\n",
        "iloc[:,:-2] : locate indexes .. all row, all column except the last 2\n",
        "iloc[:,-1] : all rows, only last column\n",
        "\n",
        ": -> range\n",
        "-1 -> last column\n",
        "2:-1 -> from 2 to excluding the last column\n"
      ],
      "metadata": {
        "id": "hnzG8rIdhaQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#read the dataset from file using pandas and store it in a variable\n",
        "dataset= pd.read_csv(\"Data.csv\")\n",
        "\n",
        "#locate the values index wise from the dataset\n",
        "X= dataset.iloc[:,:-1].values\n",
        "Y= dataset.iloc[:,-1].values"
      ],
      "metadata": {
        "id": "pkQ2guLaeNTK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI6y6vRTtjFP",
        "outputId": "76ec8884-8808-4b08-8c70-5fbff04c7659"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqrJE1OPtjk1",
        "outputId": "c4b1255a-d25a-489a-cf1a-cdf0f371ce24"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Either delete the missing data if there is huge datset\n",
        "2. Handle it properly by replacing it with some value based on some strategy\n",
        "(mean/media/most occured value/some specified constant)\n",
        "\n",
        "scikit-learn -> simpleImputer -> fill/replaces missing data\n",
        "import the SimpleImputer class from the sklearn.impute module in scikit-learn, a popular machine learning library in Python.\n",
        "\n",
        "Fit method: evaluates the mean value and identifies the places where there is missing value,\n",
        "Transform: replaces the missing value with average"
      ],
      "metadata": {
        "id": "pHfirA3SKJ_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "#Instance of class 'SimpleImputer'\n",
        "#SimpleImputer(which values to replace, with what values to replace)\n",
        "imputer = SimpleImputer(missing_values=np.nan,strategy='mean')\n",
        "\n",
        "#Connect/Apply this Imputer to our matrix/datset\n",
        "\n",
        "# the attribute in fit method, should only have numeric values not text or categorical data, hence we removed country column\n",
        "imputer.fit(X[:,1:3])\n",
        "\n",
        "#transform method - same atrribute as fit\n",
        "#tranform method will now returns the matrix X with new replaced values\n",
        "X[:,1:3]=imputer.transform(X[:,1:3])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o3gLaf5JuC13"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psA73EyYYZRm",
        "outputId": "e9ba4fb4-9b4b-4053-e231-90962ef7c5f5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriG6VzVSjcK"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhSpdQWeSsFh"
      },
      "source": [
        "### Encoding the Independent Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Country column : france, spain, germany..this   is a categorical data\n",
        "\n",
        "ColumnTransformer: allows you to apply different preprocessing steps to different columns of your dataset, especially when the data is a mix of nuerical and categorical data.\n",
        "lets you define a pipeline that applies:\n",
        "\n",
        "1. Imputation to numerical columns\n",
        "2. Encoding to categorical columns\n",
        "3. Scaling to selected columns ...all in one go\n",
        "\n",
        "StandardScaler:It standardizes numerical features by removing the mean and scaling to unit variance.\n",
        "\n",
        "OneHotEncoder: It converts categorical variables into a format that can be provided to ML algorithms.\n",
        "Convert them into numbers using 'OneHotEncoding'\n",
        "OneHotEncoding: creates binary vector\n",
        "France:[1,0,0]\n",
        "spain:[0,1,0]\n",
        "Germany:[0,0,1]\n"
      ],
      "metadata": {
        "id": "erIXxusdgx_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#is used to import the ColumnTransformer class from sklearn.compose in scikit-learn.\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "#create an object/instance of ColumnTransformer class\n",
        "#One hot encoding creates binary vector, (ie) get get binary data for the categorical values\n",
        "#create vector of size to the number of values .. there it is three(spain, france, germany)\n",
        "#transfor[(kind of tranformation, what kind of transforamtion, on what columns)]\n",
        "ct=ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[0])], remainder='passthrough')\n",
        "\n",
        "#Connect with Matrix Feature X\n",
        "# output of fit_Transform is not a numpy array, but fror our future machine learning steps of traning set we want X as numpy array , hence we convert the output of fit_transform into numpy array\n",
        "X=np.array(ct.fit_transform(X))\n",
        "\n"
      ],
      "metadata": {
        "id": "MWb69mDKhBME"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h38rUqZcipCb",
        "outputId": "6b8a616b-4158-419c-d491-50962c9da0bd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXh8oVSITIc6"
      },
      "source": [
        "### Encoding the Dependent Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LabelEncoder is used to convert categorical labels (strings) into numeric values.(but only if the categories have an ordinal relationship[link text]"
      ],
      "metadata": {
        "id": "FvU_WM1Pzhl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#create an instance of LabelEncoder (only 2 values-> 0,1-> yes, No)\n",
        "le = LabelEncoder()\n",
        "\n",
        "#link labelencoder with Y\n",
        "# we need not convert this into numpy array, as its the dependent variable, it need not be a numpy array\n",
        "Y=le.fit_transform(Y)\n"
      ],
      "metadata": {
        "id": "jY2D10Zlzevj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y)"
      ],
      "metadata": {
        "id": "3fo1e7k0zg1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Should we apply feature scaling before after splitting dataset into training set ad test set\n",
        "\n",
        "Test set should be like a new set to evaluate our model's accuracy\n",
        "apply feature scaling on test set is like information leakage that is knowing the customer data in production prehand, with this the results might not be accurate."
      ],
      "metadata": {
        "id": "8DwZ0TLb2a0F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kMJyxvzx1Tdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "icnj47Zc1TaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xQ8XUbrX1TXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "11d2a8Wd1TUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Feature Scaling"
      ]
    }
  ]
}